{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language (ASL) recognition\n",
    "An Artificial Intelligence project by Emile GATIGNON and Martin RAMPONT\n",
    "\n",
    "> \"Artificial Intelligence\" - Course N° 12721 at Hanyang University with professor 백성용 / Sungyong Baik\n",
    "> \n",
    "> Spring Semester 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "%pip install tensorflow==2.12 scikit-learn opencv-python mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import cv2\n",
    "import hashlib\n",
    "import json\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "# Dataset\n",
    "# ? Dataset source : https://www.kaggle.com/datasets/risangbaskoro/wlasl-processed\n",
    "data_url = r'https://storage.googleapis.com/kaggle-data-sets/1589971/2632847/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230528%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230528T101545Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3d32a329ea1d2da65583832ef994b2b0ada5e17e04f938afff9fdd04fca8643b22b889accfc807458f1f8616b28cc2b0412f464649c851bdd2d0d6d4bfca08bd85f37f0be1653fdd3c85fb44b6abf81faf3051ca1eb817c1a52158574d1545d7723f498008fb2b151c5a1a2ab855299e72727c9ecc3138965c81e33660024625a3779e065613c78c7520913c5279bdbe392010d66bab023509f1a1a792fb5567ddf7865fdb51f354fd737ac202a07d02481ec04eb5e26f44a94aa942d4dd395bd1f5984ba5eb60b46a80a5cd33b7229558c69bfd524c3e7ec49b18150956e3b0b96849ec2270cadb458b38f9cee415722a91fbbecc3acebabec79d25016c0217'\n",
    "data_path = r'downloads/data'\n",
    "videos_folder = r'videos'\n",
    "landmarks_folder = r'landmarks'\n",
    "data_description_file = r'WLASL_v0.3.json'\n",
    "labels_file = r'labels.json'\n",
    "\n",
    "# Dataset formating\n",
    "inclued_landmarks = {\n",
    "    \"include_face\": False,\n",
    "    \"include_pose\": True,\n",
    "    \"include_righth\": True,\n",
    "    \"include_lefth\": True,\n",
    "}\n",
    "\n",
    "\n",
    "labels_file_path = os.path.join(data_path, labels_file)\n",
    "data_description_file_path = os.path.join(data_path, data_description_file)\n",
    "videos_folder_path = os.path.join(data_path, videos_folder)\n",
    "landmarks_folder_path = os.path.join(data_path, landmarks_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download and extraction\n",
    "data_zip_hash = '1b8198227bb3de21de639146016a7368'\n",
    "\n",
    "if os.path.isdir(landmarks_folder_path):\n",
    "    print(\"Landmarks found, skipping download\")\n",
    "elif os.path.isfile(data_description_file_path) \\\n",
    "        and os.path.isdir(videos_folder_path):\n",
    "    print(\"Data already unpacked, skipping\")\n",
    "else:\n",
    "    downloaded_hash = ''\n",
    "    if os.path.isfile(data_path + '.zip'):\n",
    "        print(\"Data already downloaded, checking intergrity...\")\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(data_path + '.zip', \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        downloaded_hash = hash_md5.hexdigest()\n",
    "\n",
    "    if data_zip_hash != downloaded_hash:\n",
    "        def report_hook(count, block_size, total_size):\n",
    "            percentage = (count * block_size / total_size) * 100\n",
    "            print(f\"Downloading data... {percentage:.2f}%\", end='\\r')\n",
    "        urllib.request.urlretrieve(data_url, data_path + '.zip', reporthook=report_hook)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"Downloaded zip integrity ok\")\n",
    "\n",
    "    with zipfile.ZipFile(data_path + '.zip', 'r') as zip_ref:\n",
    "        total_files = len(zip_ref.namelist())\n",
    "        extracted_files = 0\n",
    "        for file in zip_ref.namelist():\n",
    "            zip_ref.extract(file, data_path)\n",
    "\n",
    "            extracted_files += 1\n",
    "            progress = (extracted_files / total_files) * 100\n",
    "            print(f\"Extracting... {progress:.2f}%\", end='\\r')\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark detection - variables and functions\n",
    "\n",
    "MP_HOLISTIC = mp.solutions.holistic\n",
    "MP_DRAWING = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "def mediapipe_detection(image: cv2.Mat, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    MP_DRAWING.draw_landmarks(image, results.face_landmarks, MP_HOLISTIC.FACEMESH_CONTOURS,\n",
    "                              MP_DRAWING.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "                              MP_DRAWING.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)\n",
    "                              )\n",
    "    # Draw pose connections\n",
    "    MP_DRAWING.draw_landmarks(image, results.pose_landmarks, MP_HOLISTIC.POSE_CONNECTIONS,\n",
    "                              MP_DRAWING.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "                              MP_DRAWING.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)\n",
    "                              )\n",
    "    # Draw left hand connections\n",
    "    MP_DRAWING.draw_landmarks(image, results.left_hand_landmarks, MP_HOLISTIC.HAND_CONNECTIONS,\n",
    "                              MP_DRAWING.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              MP_DRAWING.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "                              )\n",
    "    # Draw right hand connections\n",
    "    MP_DRAWING.draw_landmarks(image, results.right_hand_landmarks, MP_HOLISTIC.HAND_CONNECTIONS,\n",
    "                              MP_DRAWING.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              MP_DRAWING.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                              )\n",
    "\n",
    "\n",
    "def extract_landmarks(results) -> np.ndarray:\n",
    "    \"\"\"Transforms the results from a mediapipe process to a NumPy Array\n",
    "\n",
    "    Args:\n",
    "        results: Results from a mediapipe process\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Vectorized results, missing landmarks are representend as numpy.nan\n",
    "\n",
    "        results.shape = (4,)\n",
    "\n",
    "        results[0].shape = (468, 3), results[1].shape = (33, 3),\n",
    "        results[2].shape = (21, 3), results[3].shape = (21, 3)\n",
    "    \"\"\"\n",
    "    face_landmarks = np.zeros((468, 3))\n",
    "    face_landmarks.fill(np.nan)\n",
    "    if results.face_landmarks != None:\n",
    "        for i, landmark in enumerate(results.face_landmarks.landmark):\n",
    "            face_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        face_landmarks.fill(np.nan)\n",
    "\n",
    "    pose_landmarks = np.zeros((33, 3))\n",
    "    pose_landmarks.fill(np.nan)\n",
    "    if results.pose_landmarks != None:\n",
    "        for i, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            pose_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        pose_landmarks.fill(np.nan)\n",
    "\n",
    "    left_hand_landmarks = np.zeros((21, 3))\n",
    "    left_hand_landmarks.fill(np.nan)\n",
    "    if results.left_hand_landmarks != None:\n",
    "        for i, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "            left_hand_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        left_hand_landmarks.fill(np.nan)\n",
    "\n",
    "    right_hand_landmarks = np.zeros((21, 3))\n",
    "    right_hand_landmarks.fill(np.nan)\n",
    "    if results.right_hand_landmarks != None:\n",
    "        for i, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "            right_hand_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        right_hand_landmarks.fill(np.nan)\n",
    "\n",
    "    return np.array([face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks], dtype=object)\n",
    "\n",
    "\n",
    "def video_to_landmarks(video_path: str, display: bool = False) -> np.ndarray:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    landmark_frames = np.zeros((int(cap.get(cv2.CAP_PROP_FRAME_COUNT))), dtype=np.ndarray)\n",
    "    i = 0\n",
    "    with MP_HOLISTIC.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            landmark_frames[i] = extract_landmarks(results)\n",
    "            i += 1\n",
    "\n",
    "            if display:\n",
    "                draw_styled_landmarks(image, results)\n",
    "                cv2.imshow(f\"Converting '{video_path}'...\", image)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    display = False\n",
    "                    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return landmark_frames, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark detection - dataset conversion\n",
    "display_conversion = False\n",
    "\n",
    "\n",
    "if not os.path.exists(landmarks_folder_path):\n",
    "    os.mkdir(landmarks_folder_path)\n",
    "video_count = len(os.listdir(videos_folder_path))\n",
    "skipped = 0\n",
    "\n",
    "try:\n",
    "    for i, video_path in enumerate(os.listdir(videos_folder_path)):\n",
    "        progress = 100 * i / video_count\n",
    "        print(f\"Generating landmarks: {progress:6.2f}% ({skipped:5} skipped) -> {video_path}\", end='\\r')\n",
    "\n",
    "        array_path = os.path.join(landmarks_folder_path, video_path[:-4]) + '.npy'\n",
    "        if video_path.endswith('.mp4') and not os.path.isfile(array_path):\n",
    "            landmarks, display_conversion = video_to_landmarks(os.path.join(videos_folder_path, video_path), display_conversion)\n",
    "            np.save(array_path, landmarks)\n",
    "        else:\n",
    "            skipped += 1\n",
    "except KeyboardInterrupt:\n",
    "    count = len(os.listdir(landmarks_folder_path))\n",
    "    progress = 100 * count / video_count\n",
    "    print(f\"Interrupted landmark generation at {progress:6.2f}% -> {skipped} skipped, {count - skipped} generated\")\n",
    "print(f\"Finished landmark generation -> {skipped} skipped, {len(os.listdir(landmarks_folder_path)) - skipped} generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels extraction\n",
    "\n",
    "if os.path.isfile(labels_file_path):\n",
    "    print(\"Labels already generated, skipping\")\n",
    "else:\n",
    "    labels = {}\n",
    "\n",
    "    print(\"Loading data description and extracting labels...\")\n",
    "    with open(data_description_file_path, \"r\") as data_descriptor:\n",
    "        data_desc = json.load(data_descriptor)\n",
    "        for entry in data_desc:\n",
    "            for instance in entry['instances']:\n",
    "                labels[instance['video_id']] = entry['gloss']\n",
    "\n",
    "    print(\"Saving labels...\")\n",
    "    with open(labels_file_path, \"w\") as labels_container:\n",
    "        json.dump(labels, labels_container, indent=4)\n",
    "\n",
    "    print(\"Labels generated.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect\n",
    "file = open(os.path.join(data_path,r'wlasl_class_list.txt'), 'r')\n",
    "lines = file.readlines()\n",
    "actions = [line.strip().split('\\t', maxsplit=1)[1] for line in lines]\n",
    "\n",
    "# Python list --> numpy array\n",
    "actions = np.array(actions)\n",
    "\n",
    "# numpy array --> dictionnary\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "max_timesteps = 0\n",
    "\n",
    "\n",
    "def vectorize_landmark(landmark: np.ndarray, *, include_face: bool = True, include_pose: bool = True,\n",
    "                       include_righth: bool = True, include_lefth: bool = True) -> np.ndarray:\n",
    "    return np.concatenate(landmark[[include_face, include_pose, include_lefth, include_righth]]).flatten()\n",
    "\n",
    "\n",
    "def vectorize_landmark_frames(landmark_frames: np.ndarray, *, include_face: bool = True, include_pose: bool = True,\n",
    "                              include_righth: bool = True, include_lefth: bool = True) -> np.ndarray:\n",
    "    for i, landmarks in enumerate(landmark_frames):\n",
    "        if type(landmarks) is np.ndarray:\n",
    "            landmark_frames[i] = vectorize_landmark(landmarks, include_face=include_face, include_pose=include_pose,\n",
    "                                                    include_lefth=include_lefth, include_righth=include_righth)\n",
    "        else:\n",
    "            landmark_frames[i] = \\\n",
    "                vectorize_landmark(np.array([np.zeros((468, 3)), np.zeros((33, 3)), np.zeros((21, 3)), np.zeros((21, 3))]),\n",
    "                                   include_face=include_face, include_pose=include_pose,\n",
    "                                   include_lefth=include_lefth, include_righth=include_righth)\n",
    "    return landmark_frames\n",
    "\n",
    "\n",
    "with open(labels_file_path, \"r\") as labels_json:\n",
    "    labels_dic = json.load(labels_json)\n",
    "\n",
    "i = 0\n",
    "for sequence_id in os.listdir(landmarks_folder_path):\n",
    "    video_id = sequence_id[:-4]\n",
    "    if (i < 5000):\n",
    "        window = np.load(os.path.join(landmarks_folder_path, sequence_id), allow_pickle=True)\n",
    "        if (window.size > max_timesteps):\n",
    "            max_timesteps = window.size\n",
    "        sequences.append(vectorize_landmark_frames(window))\n",
    "        labels.append(label_map[labels_dic[video_id]])\n",
    "    i = i+1\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "print('max_timesteps = ', max_timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences)\n",
    "print(labels)\n",
    "print(sequences.shape)\n",
    "print(sequences.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_number_jaggedArray(arr):\n",
    "    dimensions = 0\n",
    "    arr_copy = arr.copy()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            _ = arr_copy[0]\n",
    "            dimensions += 1\n",
    "            arr_copy = arr_copy[0]\n",
    "        except (TypeError, IndexError):\n",
    "            break\n",
    "\n",
    "    return dimensions\n",
    "\n",
    "\n",
    "def find_jagged_dimension(arr):\n",
    "    if arr.ndim != dim_number_jaggedArray(arr):\n",
    "        return arr.ndim+1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "print(dim_number_jaggedArray(sequences))\n",
    "print(find_jagged_dimension(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tentative padding manuel - NOT FULLY OPERATIONAL YET\n",
    "# # Padding\n",
    "# padded_sequences = []\n",
    "# for sequence in sequences:\n",
    "#     if sequence.shape[0] < max_timesteps:\n",
    "#         padded_sequence = np.pad(sequence, ((0, max_timesteps - sequence.shape[0]), (0, 0)), mode='constant')\n",
    "#     else:\n",
    "#         padded_sequence = sequence[:max_timesteps, :]\n",
    "#     padded_sequences.append(padded_sequence)\n",
    "\n",
    "# # Convertir en tableau numpy\n",
    "# padded_sequences = np.array(padded_sequences)\n",
    "# labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sequences.size):\n",
    "    sequences[i] = sequences[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[9])\n",
    "# find_jagged_dimension(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_X_train=tf.ragged.constant(X_train)\n",
    "print(r_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan with 0. float64 in the ragged tensor to be able to compute loss and accuracy in the model fitting\n",
    "def replace_nan_with_zero(ragged_tensor):\n",
    "    nan_mask = tf.math.is_nan(ragged_tensor.values)\n",
    "    zero_tensor = tf.zeros_like(ragged_tensor.values, dtype=tf.float64)\n",
    "    filled_values = tf.where(nan_mask, zero_tensor, ragged_tensor.values)\n",
    "    filled_ragged_tensor = tf.RaggedTensor.from_row_lengths(filled_values, ragged_tensor.row_lengths())\n",
    "    return filled_ragged_tensor\n",
    "\n",
    "no_nan_r_X_train=replace_nan_with_zero(r_X_train)\n",
    "\n",
    "no_nan_r_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train).astype(int)\n",
    "y_train_cat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard to visualize model performances\n",
    "\n",
    "log_dir_path = r'logs'\n",
    "print(os.path.exists(log_dir_path))\n",
    "tb_callback = TensorBoard(log_dir='C:\\\\Users\\\\Elève\\\\Documents\\\\GitHub\\\\Sign-Language--ASL--recognition\\\\logs')\n",
    "if os.access('C:\\\\Users\\\\Elève\\\\Documents\\\\GitHub\\\\Sign-Language--ASL--recognition\\\\logs', os.W_OK):\n",
    "    print(\"Le répertoire a les permissions d'accès en écriture.\")\n",
    "else:\n",
    "    print(\"Le répertoire n'a pas les permissions d'accès en écriture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(data_path, 'checkpoint'),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Input(shape=(None, 1629), ragged=True))\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training with TensorBoard callback = data for visualization\n",
    "model.fit(no_nan_r_X_train, y_train_cat, epochs=2000, callbacks=[cp_callback])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
