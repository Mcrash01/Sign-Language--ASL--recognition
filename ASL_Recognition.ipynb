{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language (ASL) recognition\n",
    "An Artificial Intelligence project by Emile GATIGNON and Martin RAMPONT\n",
    "\n",
    "> \"Artificial Intelligence\" - Course N° 12721 at Hanyang University with professor 백성용 / Sungyong Baik\n",
    "> \n",
    "> Spring Semester 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "%pip install tensorflow==2.12 opencv-python mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Masking\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuDNN acceleration (if GPU available --> Nvidia CUDA drivers required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landmarks detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "labels_map_file = r'labels_map.json'\n",
    "checkpoint_file = r'ckpt.hdf5'\n",
    "guess_interval = 60\n",
    "\n",
    "# Calculated params\n",
    "total_param_count = (33 + 21 + 21) * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark detection - variables and functions\n",
    "\n",
    "MP_HOLISTIC = mp.solutions.holistic\n",
    "MP_DRAWING = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "def mediapipe_detection(image: cv2.Mat, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    # MP_DRAWING.draw_landmarks(image, results.face_landmarks, MP_HOLISTIC.FACEMESH_CONTOURS,\n",
    "    #                           MP_DRAWING.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "    #                           MP_DRAWING.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)\n",
    "    #                           )\n",
    "    # Draw pose connections\n",
    "    MP_DRAWING.draw_landmarks(image, results.pose_landmarks, MP_HOLISTIC.POSE_CONNECTIONS,\n",
    "                              MP_DRAWING.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "                              MP_DRAWING.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)\n",
    "                              )\n",
    "    # Draw left hand connections\n",
    "    MP_DRAWING.draw_landmarks(image, results.left_hand_landmarks, MP_HOLISTIC.HAND_CONNECTIONS,\n",
    "                              MP_DRAWING.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              MP_DRAWING.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)\n",
    "                              )\n",
    "    # Draw right hand connections\n",
    "    MP_DRAWING.draw_landmarks(image, results.right_hand_landmarks, MP_HOLISTIC.HAND_CONNECTIONS,\n",
    "                              MP_DRAWING.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              MP_DRAWING.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "                              )\n",
    "\n",
    "\n",
    "def extract_landmarks(results) -> np.ndarray:\n",
    "    \"\"\"Transforms the results from a mediapipe process to a NumPy Array\n",
    "\n",
    "    Args:\n",
    "        results: Results from a mediapipe process\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Vectorized results, missing landmarks are representend as numpy.nan\n",
    "\n",
    "        results.shape = (4,)\n",
    "\n",
    "        results[0].shape = (468, 3), results[1].shape = (33, 3),\n",
    "        results[2].shape = (21, 3), results[3].shape = (21, 3)\n",
    "    \"\"\"\n",
    "    face_landmarks = np.zeros((468, 3))\n",
    "    face_landmarks.fill(np.nan)\n",
    "    if results.face_landmarks != None:\n",
    "        for i, landmark in enumerate(results.face_landmarks.landmark):\n",
    "            face_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        face_landmarks.fill(np.nan)\n",
    "\n",
    "    pose_landmarks = np.zeros((33, 3))\n",
    "    pose_landmarks.fill(np.nan)\n",
    "    if results.pose_landmarks != None:\n",
    "        for i, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            pose_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        pose_landmarks.fill(np.nan)\n",
    "\n",
    "    left_hand_landmarks = np.zeros((21, 3))\n",
    "    left_hand_landmarks.fill(np.nan)\n",
    "    if results.left_hand_landmarks != None:\n",
    "        for i, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "            left_hand_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        left_hand_landmarks.fill(np.nan)\n",
    "\n",
    "    right_hand_landmarks = np.zeros((21, 3))\n",
    "    right_hand_landmarks.fill(np.nan)\n",
    "    if results.right_hand_landmarks != None:\n",
    "        for i, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "            right_hand_landmarks[i] = landmark.x, landmark.y, landmark.z\n",
    "    else:\n",
    "        right_hand_landmarks.fill(np.nan)\n",
    "\n",
    "    return np.array([face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks], dtype=object)\n",
    "\n",
    "\n",
    "def video_to_landmarks(video_path: str, display: bool = False) -> np.ndarray:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    landmark_frames = np.zeros((int(cap.get(cv2.CAP_PROP_FRAME_COUNT))), dtype=np.ndarray)\n",
    "    i = 0\n",
    "    with MP_HOLISTIC.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            landmark_frames[i] = extract_landmarks(results)\n",
    "            i += 1\n",
    "\n",
    "            if display:\n",
    "                draw_styled_landmarks(image, results)\n",
    "                cv2.imshow(f\"Converting '{video_path}'...\", image)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    display = False\n",
    "                    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return landmark_frames, display\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading - functions\n",
    "def vectorize_landmark(landmark: np.ndarray, *, include_face: bool = True, include_pose: bool = True,\n",
    "                       include_righth: bool = True, include_lefth: bool = True) -> np.ndarray:\n",
    "    return np.nan_to_num(\n",
    "        np.concatenate(landmark[[include_face, include_pose, include_lefth, include_righth]]).flatten(),\n",
    "        nan=0.0,\n",
    "        copy=False\n",
    "    )\n",
    "\n",
    "\n",
    "def vectorize_landmark_frames(landmark_frames: np.ndarray, *, include_face: bool = True, include_pose: bool = True,\n",
    "                              include_righth: bool = True, include_lefth: bool = True) -> np.ndarray:\n",
    "    for i, landmarks in enumerate(landmark_frames):\n",
    "        if type(landmarks) is np.ndarray:\n",
    "            landmark_frames[i] = vectorize_landmark(landmarks, include_face=include_face, include_pose=include_pose,\n",
    "                                                    include_lefth=include_lefth, include_righth=include_righth)\n",
    "        else:\n",
    "            landmark_frames[i] = \\\n",
    "                vectorize_landmark(np.array([np.zeros((468, 3)), np.zeros((33, 3)), np.zeros((21, 3)), np.zeros((21, 3))]),\n",
    "                                   include_face=include_face, include_pose=include_pose,\n",
    "                                   include_lefth=include_lefth, include_righth=include_righth)\n",
    "    return landmark_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels loading\n",
    "with open(labels_map_file, \"r\") as labels_map_json:\n",
    "    labels_map = json.load(labels_map_json)\n",
    "labels_list = np.array(list(labels_map.keys()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Input(shape=(guess_interval, total_param_count)))\n",
    "model.add(Masking(mask_value=0.))\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(labels_list), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.load_weights(checkpoint_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]\n",
    "\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85 + num * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return output_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "try:\n",
    "    with MP_HOLISTIC.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            # 1. Input\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # 2. Prediction logic\n",
    "            keypoints = vectorize_landmark(extract_landmarks(results), include_face=False)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-guess_interval:]\n",
    "\n",
    "            if len(sequence) == guess_interval:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(labels_list[np.argmax(res)])\n",
    "                predictions.append(np.argmax(res))\n",
    "\n",
    "            # 3. Viz logic\n",
    "                if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                    if res[np.argmax(res)] > threshold:\n",
    "                        if len(sentence) > 0:\n",
    "                            if labels_list[np.argmax(res)] != sentence[-1]:\n",
    "                                sentence.append(labels_list[np.argmax(res)])\n",
    "                        else:\n",
    "                            sentence.append(labels_list[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "            cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (3, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow(\"ASL Recognition Demo [exit with 'Q']\", image)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
